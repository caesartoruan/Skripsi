{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyWnWm1WASWp"
      },
      "outputs": [],
      "source": [
        "#Lib for Preprocessing and load data \n",
        "import os\n",
        "from io import open\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "#Lib for Modeling\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "torch.manual_seed(1)\n",
        "torch.set_printoptions(precision=30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = [[-0.2, -0.1, 0.1, 1.3],\n",
        "        [-0.9, 1.4, -0.5, 0.6],\n",
        "        [-1.3, 1.2, 0.8, 0.7],\n",
        "        [-0.7, -0.1, -0.1, -2.1],\n",
        "        [0.6, 0.2, -0.7, 0.5]]"
      ],
      "metadata": {
        "id": "KiwZ4Y6sdm4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor(input)"
      ],
      "metadata": {
        "id": "urfyT1hVAenf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "som9RcFaAe3N",
        "outputId": "d1c0c865-d329-45f7-a298-91f8ffcf72dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.200000002980232238769531250000, -0.100000001490116119384765625000,\n",
              "          0.100000001490116119384765625000,  1.299999952316284179687500000000],\n",
              "        [-0.899999976158142089843750000000,  1.399999976158142089843750000000,\n",
              "         -0.500000000000000000000000000000,  0.600000023841857910156250000000],\n",
              "        [-1.299999952316284179687500000000,  1.200000047683715820312500000000,\n",
              "          0.800000011920928955078125000000,  0.699999988079071044921875000000],\n",
              "        [-0.699999988079071044921875000000, -0.100000001490116119384765625000,\n",
              "         -0.100000001490116119384765625000, -2.099999904632568359375000000000],\n",
              "        [ 0.600000023841857910156250000000,  0.200000002980232238769531250000,\n",
              "         -0.699999988079071044921875000000,  0.500000000000000000000000000000]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = [0]"
      ],
      "metadata": {
        "id": "bMODhUTUeAx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = torch.tensor([target])"
      ],
      "metadata": {
        "id": "661r_RWkBY4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwotOYPIHG_B",
        "outputId": "7c7d9d78-d2af-46a6-8976-0b03ffea6e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GUnPbI6AgK0",
        "outputId": "3b683508-ffcf-4fc4-833c-2512146c6254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(hidden_size, \n",
        "                          1,\n",
        "                          bidirectional=True)\n",
        "        #self.drop = nn.Dropout(p=0.5)\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "          if 'weight' or 'bias' in name:\n",
        "            nn.init.ones_(param)\n",
        "        self.fc = nn.Linear(2, 1)\n",
        "        nn.init.ones_(self.fc.weight)\n",
        "        nn.init.ones_(self.fc.bias)\n",
        "        \n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "\n",
        "        print('\\n===EMBEDDED RESULT===\\n', input)\n",
        "        # csv_emb = input.tolist()\n",
        "        # csv_emb = convertDecimal(csv_emb)\n",
        "\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        #packed = nn.utils.rnn.pack_padded_sequence(input, input_lengths)\n",
        "        #print('\\n===PACK PADDED SEQUENCE===\\n', packed)\n",
        "        \n",
        "        print(\"\\n=====LSTM's WEIGHTS and BIAS=====\\n\\n\",self.lstm.all_weights)\n",
        "        print(\"\\n=====Linear's WEIGHTS=====\\n\\n\",self.fc.weight)\n",
        "        print(\"\\n=====Linear's BIAS=====\\n\\n\",self.fc.bias)\n",
        "        outputs, hidden = self.lstm(input, hidden)\n",
        "        \n",
        "        # print(\"outputs\", outputs)\n",
        "        # Unpack padding\n",
        "        #coba = outputs[:,0,4]\n",
        "        #print(\"\\n=====GRU coba=====\\n\\n\", coba)\n",
        "\n",
        "        #outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        print(\"\\n=====LSTM OUTPUTS=====\\n\\n\",outputs)\n",
        "        print(\"\\n=====LSTM HIDDEN=====\\n\\n\",hidden)\n",
        "        \n",
        "        # Sum bidirectional GRU outputs\n",
        "        \n",
        "\n",
        "        #outputs = outputs[:,:self.hidden_size] + outputs[:,self.hidden_size:]\n",
        "        #print(\"\\n=====COMBINED GRU OUTPUTS=====\\n\\n\",outputs)\n",
        "\n",
        "\n",
        "        c = torch.cat((hidden[0], hidden[1]), 0)\n",
        "        print('new hidden', c)\n",
        "\n",
        "        print('new hidden', c.shape)\n",
        "        print(' hidden', c[0].item())\n",
        "\n",
        "        # h = [[c[0].item(), c[1].item(), c[2].item(), c[3].item()]]\n",
        "        h = [[c[0].item(), c[1].item()]]\n",
        "\n",
        "\n",
        "        print('h', h)\n",
        "\n",
        "        th = torch.tensor(h)\n",
        "\n",
        "        print('th', th)\n",
        "        print('th', th.shape)\n",
        "\n",
        "        th = th.to(torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "        # output_dropped = self.drop(c)\n",
        "        # print('new hidden', output_dropped)\n",
        "\n",
        "\n",
        "\n",
        "        # hidden = self.fc(c)\n",
        "        hidden = self.fc(th)\n",
        "\n",
        "        print('Perhitungan Linear : ', hidden)\n",
        "\n",
        "\n",
        "        hidden = F.sigmoid(hidden)\n",
        "\n",
        "        print('Klasifikasi Label : ', hidden)\n",
        "\n",
        "        # text_squeeze = torch.squeeze(hidden, 1)\n",
        "        # final_output = torch.sigmoid(text_squeeze)\n",
        "\n",
        "\n",
        "        # hidden = F.softmax(hidden, dim=1)\n",
        "        # print(\"bi out\", outputs.shape)\n",
        "        # Return output and final hidden state\n",
        "        # return final_output\n",
        "        return hidden\n",
        "    \n",
        "    #def init_hidden(self, batch_size):\n",
        "     #   weight = next(self.parameters()).data\n",
        "      #  hidden = (weight.new(1, batch_size, self.hidden_size).zero_(),\n",
        "       #           weight.new(1, batch_size, self.hidden_size).zero_())\n",
        "       # return hidden"
      ],
      "metadata": {
        "id": "6KCgyBSZAhqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input, target, net, optimizer, criterion):\n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input.to(device)\n",
        "    target_variable = target.to(device)\n",
        "    # input_variable = input\n",
        "    # target_variable = target\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    print('input_variable', input_variable)\n",
        "    hidden = net(input_variable)\n",
        "\n",
        "    print('Hidden : ', hidden)\n",
        "    print('Target : ', target)\n",
        "\n",
        "    # # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    # decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    # decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    # #set decoder_hidden if using bidirectional GRU in decoder\n",
        "    # decoder_hidden = encoder_hidden[:1]\n",
        "\n",
        "    # #set decoder_hidden if using vanilla GRU in decoder\n",
        "    # # decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # # Determine if we are using teacher forcing this iteration\n",
        "    # # use_teacher_forcing = (True if torch.rand(2).tolist()[0] < teacher_forcing_ratio \n",
        "    # #                       else False)\n",
        "    # # print('Teacher forcing status:',use_teacher_forcing)\n",
        "\n",
        "    # # Forward batch of sequences through decoder one time step at a time\n",
        "    # # if use_teacher_forcing:\n",
        "    # for t in range(max_target_len):\n",
        "    #     decoder_output, decoder_hidden = decoder(\n",
        "    #         decoder_input, decoder_hidden, encoder_outputs\n",
        "    #     )\n",
        "    #     # Teacher forcing: next input is current target\n",
        "    #     decoder_input = target_variable[t].view(1, -1)\n",
        "    #     # Calculate and accumulate loss\n",
        "    #     decoder_output = torch.log(decoder_output)\n",
        "    #     loss = criterion(decoder_output,\n",
        "    #                                     target_variable[t])\n",
        "    #     print('targ',target_variable[t])\n",
        "    #     print('loss',loss)\n",
        "    #     loss += loss\n",
        "    #     print_losses.append(loss.item())\n",
        "\n",
        "    loss_ = criterion(hidden.float(), target.float())\n",
        "    print('targ',target)\n",
        "    print('loss',loss_)\n",
        "    loss += loss_\n",
        "    print_losses.append(loss.item())\n",
        "\n",
        "    # else:\n",
        "    #   for t in range(max_target_len):\n",
        "    #       decoder_output, decoder_hidden = decoder(\n",
        "    #           decoder_input, decoder_hidden, encoder_outputs\n",
        "    #       )\n",
        "    #       # No teacher forcing: next input is decoder's own current output\n",
        "    #       _, topi = decoder_output.topk(1)\n",
        "    #       decoder_input = torch.LongTensor(\n",
        "    #           [[topi[i][0] for i in range(batch_size)]])\n",
        "    #       decoder_input = decoder_input.to(device)\n",
        "    #       # Calculate and accumulate loss\n",
        "    #       decoder_output = torch.log(decoder_output)\n",
        "    #       loss = criterion(decoder_output,\n",
        "    #                                       target_variable[t])\n",
        "    #       print('targ',target_variable[t])\n",
        "    #       print('loss',loss)\n",
        "    #       loss += loss\n",
        "    #       print_losses.append(loss.item())\n",
        "# Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    # _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    optimizer.step()\n",
        "\n",
        "    print('loss :', loss)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "m6gXSCAoAn1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 4\n",
        "batch_size = 1\n",
        "# clip = 50.0\n",
        "# teacher_forcing_ratio = 0.1\n",
        "learning_rate = 0.01\n",
        "epoch = 1\n",
        "loss_list = [] \n",
        "start_iteration = 0\n",
        "print_loss = 0\n",
        "\n",
        "# torch.manual_seed(1)\n",
        "\n",
        "# Initialize encoder & decoder models\n",
        "net = BiLSTM(hidden_size)\n",
        "\n",
        "# Use appropriate device\n",
        "#encoder = encoder.to(device)\n",
        "\n",
        "#encoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "BBq3rh9fBmIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('precision', 7)"
      ],
      "metadata": {
        "id": "l0VT6cQEdSo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ep in range(epoch):\n",
        "  #random.shuffle(training_batches)\n",
        "  # print(\"Epoch {}\".format(ep+1))\n",
        "  #data train session \n",
        "  print('input', input)\n",
        "  loss = train(input, target, net, optimizer, criterion)\n",
        "  print('acummulative',loss)\n",
        "  print_loss += loss\n",
        "  loss_list=print_loss\n",
        "  # print(\"\\tIteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration/(len(train_batch)-1) * 100, print_loss))\n",
        "  print_loss = 0"
      ],
      "metadata": {
        "id": "DbE_oPg0DNw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1efc9b-dafa-4eec-c395-eccafac06eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input tensor([[-0.200000002980232238769531250000, -0.100000001490116119384765625000,\n",
            "          0.100000001490116119384765625000,  1.299999952316284179687500000000],\n",
            "        [-0.899999976158142089843750000000,  1.399999976158142089843750000000,\n",
            "         -0.500000000000000000000000000000,  0.600000023841857910156250000000],\n",
            "        [-1.299999952316284179687500000000,  1.200000047683715820312500000000,\n",
            "          0.800000011920928955078125000000,  0.699999988079071044921875000000],\n",
            "        [-0.699999988079071044921875000000, -0.100000001490116119384765625000,\n",
            "         -0.100000001490116119384765625000, -2.099999904632568359375000000000],\n",
            "        [ 0.600000023841857910156250000000,  0.200000002980232238769531250000,\n",
            "         -0.699999988079071044921875000000,  0.500000000000000000000000000000]])\n",
            "input_variable tensor([[-0.200000002980232238769531250000, -0.100000001490116119384765625000,\n",
            "          0.100000001490116119384765625000,  1.299999952316284179687500000000],\n",
            "        [-0.899999976158142089843750000000,  1.399999976158142089843750000000,\n",
            "         -0.500000000000000000000000000000,  0.600000023841857910156250000000],\n",
            "        [-1.299999952316284179687500000000,  1.200000047683715820312500000000,\n",
            "          0.800000011920928955078125000000,  0.699999988079071044921875000000],\n",
            "        [-0.699999988079071044921875000000, -0.100000001490116119384765625000,\n",
            "         -0.100000001490116119384765625000, -2.099999904632568359375000000000],\n",
            "        [ 0.600000023841857910156250000000,  0.200000002980232238769531250000,\n",
            "         -0.699999988079071044921875000000,  0.500000000000000000000000000000]])\n",
            "\n",
            "===EMBEDDED RESULT===\n",
            " tensor([[-0.200000002980232238769531250000, -0.100000001490116119384765625000,\n",
            "          0.100000001490116119384765625000,  1.299999952316284179687500000000],\n",
            "        [-0.899999976158142089843750000000,  1.399999976158142089843750000000,\n",
            "         -0.500000000000000000000000000000,  0.600000023841857910156250000000],\n",
            "        [-1.299999952316284179687500000000,  1.200000047683715820312500000000,\n",
            "          0.800000011920928955078125000000,  0.699999988079071044921875000000],\n",
            "        [-0.699999988079071044921875000000, -0.100000001490116119384765625000,\n",
            "         -0.100000001490116119384765625000, -2.099999904632568359375000000000],\n",
            "        [ 0.600000023841857910156250000000,  0.200000002980232238769531250000,\n",
            "         -0.699999988079071044921875000000,  0.500000000000000000000000000000]])\n",
            "\n",
            "=====LSTM's WEIGHTS and BIAS=====\n",
            "\n",
            " [[Parameter containing:\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]], requires_grad=True), Parameter containing:\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], requires_grad=True), Parameter containing:\n",
            "tensor([1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
            "tensor([1., 1., 1., 1.], requires_grad=True)], [Parameter containing:\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]], requires_grad=True), Parameter containing:\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], requires_grad=True), Parameter containing:\n",
            "tensor([1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
            "tensor([1., 1., 1., 1.], requires_grad=True)]]\n",
            "\n",
            "=====Linear's WEIGHTS=====\n",
            "\n",
            " Parameter containing:\n",
            "tensor([[1., 1.]], requires_grad=True)\n",
            "\n",
            "=====Linear's BIAS=====\n",
            "\n",
            " Parameter containing:\n",
            "tensor([1.], requires_grad=True)\n",
            "\n",
            "=====LSTM OUTPUTS=====\n",
            "\n",
            " tensor([[0.709196984767913818359375000000, 0.978670358657836914062500000000],\n",
            "        [0.920975387096405029296875000000, 0.941910266876220703125000000000],\n",
            "        [0.980218648910522460937500000000, 0.814181447029113769531250000000],\n",
            "        [0.438069611787796020507812500000, 0.104753889143466949462890625000],\n",
            "        [0.934653580188751220703125000000, 0.675995051860809326171875000000]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "\n",
            "=====LSTM HIDDEN=====\n",
            "\n",
            " (tensor([[0.934653580188751220703125000000],\n",
            "        [0.978670358657836914062500000000]], grad_fn=<SqueezeBackward1>), tensor([[2.283831834793090820312500000000],\n",
            "        [3.089032173156738281250000000000]], grad_fn=<SqueezeBackward1>))\n",
            "new hidden tensor([[0.934653580188751220703125000000],\n",
            "        [0.978670358657836914062500000000],\n",
            "        [2.283831834793090820312500000000],\n",
            "        [3.089032173156738281250000000000]], grad_fn=<CatBackward0>)\n",
            "new hidden torch.Size([4, 1])\n",
            " hidden 0.9346535801887512\n",
            "h [[0.9346535801887512, 0.9786703586578369]]\n",
            "th tensor([[0.934653580188751220703125000000, 0.978670358657836914062500000000]])\n",
            "th torch.Size([1, 2])\n",
            "Perhitungan Linear :  tensor([[2.913323879241943359375000000000]], grad_fn=<AddmmBackward0>)\n",
            "Klasifikasi Label :  tensor([[0.948501169681549072265625000000]], grad_fn=<SigmoidBackward0>)\n",
            "Hidden :  tensor([[0.948501169681549072265625000000]], grad_fn=<SigmoidBackward0>)\n",
            "Target :  tensor([[0]])\n",
            "targ tensor([[0]])\n",
            "loss tensor(2.966196298599243164062500000000, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "loss : tensor(2.966196298599243164062500000000, grad_fn=<AddBackward0>)\n",
            "acummulative tensor(2.966196298599243164062500000000, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}